{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.utils as utils\n",
    "import torch.nn.init as init\n",
    "import torch.utils.data as data\n",
    "import torchvision.utils as v_utils\n",
    "import torchvision.datasets as dset\n",
    "import torchvision.transforms as transforms\n",
    "from torch.autograd import Variable\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def conv_block_3d(in_dim, out_dim, act_fn):\n",
    "    model = nn.Sequential(\n",
    "        nn.Conv3d(in_dim, out_dim, kernel_size=3, padding=1),\n",
    "        nn.BatchNorm3d(out_dim),\n",
    "        act_fn\n",
    "    )\n",
    "    return model\n",
    "\n",
    "def conv_block_trans_3d(in_dim, out_dim, act_fn=nn.LeakyReLU(0.2, inplace=True)):\n",
    "    model = nn.Sequential(\n",
    "        nn.ConvTranspose3d(in_dim, out_dim, kernel_size=3, padding = 1, stride=2, output_padding=1),\n",
    "        nn.BatchNorm3d(out_dim),\n",
    "        act_fn\n",
    "    )\n",
    "    return model\n",
    "\n",
    "def conv_full_block3d(in_dim, out_dim = -1, act_fn=nn.LeakyReLU(0.2, inplace=True)):\n",
    "    if out_dim == -1:\n",
    "        out_dim = in_dim * 2\n",
    "    model = nn.Sequential(\n",
    "        conv_block_3d(in_dim, in_dim, act_fn),\n",
    "        conv_block_3d(in_dim, out_dim, act_fn)\n",
    "    )\n",
    "    return model\n",
    "    \n",
    "def conv_up_full_block3d(in_dim, out_dim, act_fn=nn.LeakyReLU(0.2, inplace=True)):\n",
    "    model = nn.Sequential(\n",
    "        nn.Conv3d(in_dim, out_dim, kernel_size=3, padding = 1),\n",
    "        nn.BatchNorm3d(out_dim),\n",
    "        act_fn,\n",
    "        nn.Conv3d(out_dim, out_dim, kernel_size=3, padding = 1),\n",
    "        nn.BatchNorm3d(out_dim),\n",
    "        act_fn\n",
    "        \n",
    "    )\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class UNet(nn.Module):\n",
    "    def __init__(self, in_dim=1, out_dim=1, num_of_start_filters=32):\n",
    "        super(UNet, self).__init__()\n",
    "        act_fn = nn.LeakyReLU(0.2, inplace=True)\n",
    "\n",
    "        print(\"\\n------Initiating U-Net------\\n\")\n",
    "        self.conv1 = nn.Conv3d(in_dim, num_of_start_filters, kernel_size=3, padding=1)\n",
    "        self.conv2 = nn.Conv3d(num_of_start_filters, num_of_start_filters * 2, kernel_size=3, padding=1)\n",
    "        self.pool1 = nn.MaxPool3d(kernel_size=2, stride=2, padding=0)\n",
    "                \n",
    "        self.down2 = conv_full_block3d(num_of_start_filters * 2)\n",
    "        self.pool2 = nn.MaxPool3d(kernel_size=2, stride=2, padding=0)\n",
    "        \n",
    "        self.down3 = conv_full_block3d(num_of_start_filters * 4)\n",
    "        self.pool3 = nn.MaxPool3d(kernel_size=2, stride=2, padding=0)\n",
    "        \n",
    "        self.bridge = conv_full_block3d(num_of_start_filters * 8)\n",
    "        \n",
    "        self.trans1 = conv_block_trans_3d(num_of_start_filters * 16, num_of_start_filters * 16)\n",
    "        \n",
    "        \n",
    "        self.up1 = conv_up_full_block3d(768, 256)\n",
    "        self.trans2 = conv_block_trans_3d(256, 256)\n",
    "        \n",
    "        self.up2 = conv_up_full_block3d(384, 128)\n",
    "        self.trans3 = conv_block_trans_3d(128, 128)\n",
    "        \n",
    "        self.up3 = conv_up_full_block3d(192, 64)\n",
    "        \n",
    "        self.out = nn.Conv3d(64, in_dim, kernel_size=1)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x)\n",
    "        conv2 = self.conv2(x)\n",
    "        \n",
    "        pool_1 = self.pool1(conv2)\n",
    "        \n",
    "        #print(\"after first down :\")\n",
    "        #print(pool_1.size())\n",
    "\n",
    "        down2 = self.down2(pool_1)\n",
    "        pool2 = self.pool2(down2)\n",
    "        \n",
    "        #print(\"after second down :\")\n",
    "        #print(pool2.size())\n",
    "        \n",
    "        down3 = self.down3(pool2)\n",
    "        pool3 = self.pool3(down3)\n",
    "        \n",
    "       # print(\"after third down :\")\n",
    "       # print(pool3.size())\n",
    "        \n",
    "        bridge = self.bridge(pool3)\n",
    "        \n",
    "        #print(\"after bridge :\")\n",
    "        #print(bridge.size())\n",
    "        \n",
    "        trans1 = self.trans1(bridge)\n",
    "        #print(\"after first transpose: \")\n",
    "       # print(trans1.size())\n",
    "        \n",
    "        #print(\"concatenate maps along axis 1 of size :\")\n",
    "        #print(trans1.size())\n",
    "       # print(down3.size())\n",
    "        concat1 = torch.cat([trans1, down3], dim=1)\n",
    "        \n",
    "        up1 = self.up1(concat1)\n",
    "        \n",
    "        trans2 = self.trans2(up1)\n",
    "        concat2 = torch.cat([trans2, down2], dim=1)\n",
    "        \n",
    "        up2 = self.up2(concat2)\n",
    "        \n",
    "        trans3 = self.trans3(up2)\n",
    "        \n",
    "        concat3 = torch.cat([trans3, conv2], dim=1)\n",
    "        \n",
    "        up3 = self.up3(concat3)\n",
    "        \n",
    "        #print(\"Size before 1x1x1 convolution:\")\n",
    "        #print(up3.size())\n",
    "        #print(\"Size after 1x1x1 conv:\")\n",
    "        tmp = self.out(up3)\n",
    "       # print(tmp.size())\n",
    "        #out = nn.LeakyReLU(tmp, 0.2, inplace=True)\n",
    "        \n",
    "        \n",
    "        return F.leaky_relu(tmp)\n",
    "        \n",
    "        \n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_size = 64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dice_loss(pred, target):\n",
    "    eps = 0.0001\n",
    "    pred = pred.contiguous()\n",
    "    target = target.contiguous() \n",
    "    inter = torch.dot(pred.view(-1), target.view(-1))\n",
    "    #print(pred.view(-1).size())\n",
    "    union = torch.sum(pred) + torch.sum(target) + eps\n",
    "    t = (2 * inter.float() + eps) / union.float()\n",
    "    print(\"loss :\" + str(1 - t))\n",
    "    return 1 - t"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Checking funcs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "    import numpy as np\n",
    "    data1 = Variable(torch.from_numpy(np.zeros((1, 1, 32, 64, 64))), requires_grad=True)\n",
    "    data2 = Variable(torch.from_numpy(np.ones((1, 1, 32, 64, 64))), requires_grad=True)\n",
    "    target = Variable(torch.from_numpy(np.zeros((1, 1, 32, 64, 64))))\n",
    "\n",
    "    \n",
    "    data = torch.add(data1, data2)\n",
    "    \n",
    "    data1.grad == None\n",
    "    data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss = dice_loss(data, target)\n",
    "#type(loss)\n",
    "\n",
    "print(str(loss))\n",
    "loss.backward()\n",
    "loss.grad\n",
    "data1.grad\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# End of check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "def train(args, model, device, pipeline, optimizer, epoch, loss_hist, batch_size = 1):\n",
    "    model.train()\n",
    "    \n",
    "    # [batch_size, channels, z, x, y]\n",
    "    data = np.zeros((batch_size, 1, 32, 64, 64))\n",
    "    target = np.zeros((batch_size, 1, 32, 64, 64))\n",
    "    \n",
    "    for batch_idx in range(3):\n",
    "        batch_crops = pipeline.next_batch(1)\n",
    "        \n",
    "        # create shape for batch \n",
    "        for s_num in range(batch_size):\n",
    "            data[s_num][0] = (batch_crops.get(s_num, 'images'))\n",
    "            target[s_num][0] = (batch_crops.get(s_num, 'masks'))\n",
    "            \n",
    "        data_t = Variable(torch.from_numpy(data), requires_grad=True)\n",
    "        target_t = torch.from_numpy(target)\n",
    "            \n",
    "            \n",
    "        data_t, target_t = data_t.to(device), target_t.to(device)\n",
    "        print(\"data size input: \")\n",
    "        print(data_t.size())\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        output = model(data_t.double())\n",
    "        print(\"data size output: \")\n",
    "        print(output.size())\n",
    "        \n",
    "        loss = dice_loss(output, target_t)\n",
    "        loss.backward()\n",
    "                \n",
    "        optimizer.step()\n",
    "                \n",
    "        loss_hist.append(loss)\n",
    "        print('Batch: ' + str(batch_idx) + ' Loss: ' + str(loss.item()))\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from radio.dataset import FilesIndex, Dataset, Pipeline\n",
    "from radio import CTImagesMaskedBatch as CTIMB\n",
    "\n",
    "index = FilesIndex(path='Data/subset0/*.mhd', no_ext=True) # dataset initialization\n",
    "lunaset = Dataset(index=index, batch_class=CTIMB) # download to data to RAM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "lunaset.split([0.1, 0.9])  # 10 % goes for training\n",
    "\n",
    "# load annotations from csv\n",
    "nodules_df = pd.read_csv('Data/CSVFILES/annotations.csv')\n",
    "\n",
    "pipeline = (Pipeline()\n",
    "        .load(fmt='raw') # set .raw format\n",
    "        .normalize_hu(-1000, 400)  # normalize pixel colors\n",
    "        .fetch_nodules_info(nodules_df) # extract nodules info\n",
    "        .unify_spacing(shape=(128, 256, 256), spacing=(2, 2, 2)) # set uniform scale for CT pics\n",
    "        .create_mask() # extract nodule masks\n",
    "        .sample_nodules(nodule_size=(32, 64, 64), batch_size=1) # crop nodules and generate batchers\n",
    "       )\n",
    "\n",
    "lunapipe = (lunaset.train >> pipeline)  # pass lunaset to pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(lunaset.train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = UNet().to('cpu')\n",
    "model = model.double()\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.01, momentum=0.5)\n",
    "params = list(model.parameters())\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_hist = []\n",
    "train(None, model, 'cpu', lunapipe, optimizer, 1, loss_hist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
